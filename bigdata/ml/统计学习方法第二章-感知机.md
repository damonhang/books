# 统计学习方法学习笔记第二章-感知机
感知机是二类分类的线性分类模型，输入为实例的特征向量，输出为实例的类别，取+1和-1二值，属于判别模型，是神经网络和支持向量机的基础

### 感知机模型
假设如下函数
$$f(x)=sign(w*x+b)$$
称为感知机，其中w和b为感知机模型参数，$w∈R^{n}$叫做权值或权向量 b∈R叫做偏置 w*x表示w和x的内积
> 在数学中，数量积（dot product; scalar product，也称为点积）是接受在实数R上的两个向量并返回一个实数值标量的二元运算。它是欧几里得空间的标准内积。两个向量a = [a1, a2,…, an]和b = [b1, b2,…, bn]的点积定义为：a·b=a1b1+a2b2+……+anbn,

其中：
$$sign(x)=\left\{\begin{matrix}+1, x\geq 0\\ -1, x<0\end{matrix}\right.$$

感知机的集合解释: 线性方程 w*x+b=0对应特征空间的一个超平面S，w是超平面的法向量，b是超平面的截距  
> **截距** 即平面与坐标轴相交的点到原点的距离  
> **法向量** 在空间里，向量可以看做是一个点（以原点为起始点的向量），对于分离超平面方程里的向量x，就可以看做由坐标原点到超平面任意“点”的向量法向量的大小是坐标原点到分离超平面的距离，垂直于分离超平面，方向有分离超平面决定

求感知机模型，即求得模型参数w，b

### 感知机学习策略
**数据的线性可分性**
假设训练数据线性可分，学习的目标是求得一个能将训练数据正负实例点完全正确分开的分离超平面，即确定感知机模型参数w，b，需要确定一个学习策略，**即定义(经验)损失函数并将损失函数极小化**  
损失函数自然选择是误分类点的总数，但是损失函数不是参数w,b的连续可导函数,感知机采用的是误分类点到超平面S的总距离，输入空间中任一点$x_{0}$到超平面S的距离:
$$\frac{1}{||w||}|w*x_{0}+b|$$
这里 ||w||是w的$L_{2}$范数
> **可导函数** 在微积分学中,一个实变量函数是可导函数，若其在定义域中每一点导数存在。直观上说，函数图像在其定义域每一点处是相对平滑的，不包含任何尖点、断点  
> **范数** 我们知道距离的定义是一个宽泛的概念，只要满足非负、自反、三角不等式就可以称之为距离。范数是一种强化了的距离概念，它在定义上比距离多了一条数乘的运算法则。有时候为了便于理解，我们可以把范数当作距离来理解。
在数学上，范数包括向量范数和矩阵范数，向量范数表征向量空间中向量的大小，矩阵范数表征矩阵引起变化的大小。一种非严密的解释就是，对应向量范数，向量空间中的向量都是有大小的，这个大小如何度量，就是用范数来度量的，不同的范数都可以来度量这个大小，就好比米和尺都可以来度量远近一样；对于矩阵范数，学过线性代数，我们知道，通过运算AX=B，可以将向量X变化为B，矩阵范数就是来度量这个变化大小的  
**L2范数** L2范数是我们最常见最常用的范数了，我们用的最多的度量距离欧氏距离就是一种L2范数，它的定义如下：
$$||x||_{2}=\sqrt{\sum_{i}x_{i}^{2}}$$
表示向量元素的平方和再开平方
http://blog.csdn.net/shijing_0214/article/details/51757564

对于误分类点来说$-y_{i}(w·x_{i}+b)>0$
针对训练数据集，感知机sign(w·x+b)学习的损失函数定义为
$$L(w,b)=-\sum_{x_{i}∈M}y_{i}(w·x_{i}+b)$$
其中M为误分类的集合，这个损失函数就是感知机学习的**经验损失函数**,感知机的学习策略就是在假设空间中选取使损失函数式最小的模型参数w,b

### 感知机学习算法
算法木笔爱，给定训练数据集，求参数w,b使得以下损失函数极小化问题的解
$$\underset{w,b}{min}L(w,b)=-\sum_{x_{i}∈M}y_{i}(w·x_{i}+b)$$
感知机学习算法是误分类驱动的，具体采用随机梯度下降法，假设误分类点M是固定的，那么损失函数L(w,b)的梯度由
$$\bigtriangledown_{w} L(w,b)=-\sum_{x_{i}∈M}y_{i}x_{i}$$
$$\bigtriangledown_{b} L(w,b)=-\sum_{x_{i}∈M}y_{i}x_{i}$$
即随机选取一个误分类点$(x_{i},y_{i})$，对w,b进行更新:
$w\leftarrow  w+\eta y_{i}x_{i}$
$$b\leftarrow  b+\eta y_{i}$$
式子中$\eta $是步长，或者学习率，综上所诉得到如下算法:  
针对输入训练数据集，学习率$0<\eta \leq 1$,输出w,b:感知机模型f(x)=sign(w·x+b)

* (1)选取初值$w_{0},b_{0}$
* (2)在训练集中选取数据($x_{i},y_{i}$)
* (3)如果$y_{i}(w·x)\leq 0$ 则调整$w\leftarrow  w+\eta y_{i}x_{i}$ & $b\leftarrow b+\eta y_{i}$
* (4)转至(2) 直至训练集中没有误分类点
这个学习算法几何解释:当一个实例点被误分时，调整w,b值，使分离超平面向该误分类点的一侧移动，减少误分类点与超平面之间的距离，知道该点正确分类

### 感知机算法的收敛性
对于现行可分数据集，感知机学习算法原始形式收敛，即经过有限次迭代可以得到一个将训练数据完全划分的超平面 证明忽略
### 感知机学习算法的对偶形式

