# 第四章 朴素贝叶斯法
**朴素贝叶斯法是基于贝叶斯定理是特征条件独立假设的分类方法**，对于给定的训练数据集，首先基于特征条件度；里假设学习输入/输出的联合概率分布，然后基于此模型，对于给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。
### 基本方法
假设训练数据集T有X和Y的联合概率分布P(X,Y)独立同分布产生，朴素贝叶斯法通过对训练数据及学习联合概率分布P(X,Y)，具体地，学习以下先验概率分布及条件概率分布  
先验概率分布:
$$P(y=c_{k})，k=1,2···$$
条件概率分布:
$$P(X=x|Y=c_{k})=P(X^{1}=x^{1},···X^{n}=x^{n}|Y=c_{k})$$
于是学习到联合概率分布P(X,Y)
朴素贝叶斯法史记上学习到生成数据的机制，所以属于生成模型，条件独立假设等于是说用于分类的特征在类确定的条件下都是条件独立的。
朴素贝叶斯法分类时，对给定的输入x，通过学习到的模型计算后验概率分布$P(Y=c_{k}|X=x)$,将后验概率最大的类作为x的类输出，后验概率计算根据贝叶斯定理进行
> 后验概率是信息理论的基本概念之一。在一个通信系统中，在收到某个消息之后，接收端所了解到的该消息发送的概率称为后验概率。
后验概率的计算要以先验概率为基础。后验概率可以根据通过贝叶斯公式，用先验概率和似然函数计算出来

### 后验概率最大化的含义
朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化，假设选择0-1损失函数,这时期望风险函数为
$$R_{exp}(f)=E[L(Y,f(X))]$$
>[] 表示取整函数，表示不超过X的最大正整数 

期望是对联合分布P(X,Y)取的，由此取条件期望
$$R_{exp}(f)=E_{X}\sum_{k=1}^{K}[L(c_{k},f(X))]P(c_{k}|X)$$
为了使期望风险最小化，只需要对X=x逐个最小化
$$f(x)=arg max_{y∈Y}P(y=c_{k}|X=x)$$
即根据期望风险最小化准则得到了后验概率的最大化准则，即朴素贝叶斯法所采用的原理

## 朴素贝叶斯法的参数估计

### 极大似然估计
### 贝叶斯估计
