# 统计学习方法-K近邻法
k近邻法(K-NN)是一种基本**分类与回归**方法，输入为实例的特征向量，输出为实例的类别，可以取多累。K近邻法实际上利用训练数据集对特征向量空间进行划分，并作为其分类的模型，K值的选择、距离的度量以及分类决策柜子是K近邻法的三个基本要素

## K近邻算法
(1) 根据给定的距离度量在训练集T中找好粗与x最近邻的k个点，涵盖这k个点的x的领域记作$N_{k}(X)$
(2) 在$N_{k}(X)$中根据分类决策规则(例如多数表决)决定x的类别y
$$y=arg \underset{c_{j}}{max}\sum_{x_{i}∈N_{k}(x)}I(y_{i}=c_{j})$$
其中I为指示函数,即等式成立时I为1，否则为0  
**k近邻法没有显式的学习过程**
## 模型
k近邻法中，当训练集、距离度量(如欧式距离)、k值及分类决策规则(如多数表决)确定后，对于任何一个新的输入示例，它所属的类唯一的确定  
特征空间中，每个训练实例点$x_{i}$，距离该点比其他点更近的所有点组成一个区域，叫做单元，每个训练实例点拥有一个单元，所有训练实例点的单元构成对特征空间的一个划分。
## 距离度量
k近邻模型的特征空间一般是n维向量空间R。使用的距离度量是欧式距离，也可以是其他距离，如更一般的$L_{p}$距离或者明可夫斯基距离
> 机器学习常用的五种距离度量 **欧式距离、曼哈顿距离、夹角余玄、切比雪夫距离、汉明距离**，具体定义可google查阅

## k值的选择
如果k值的选择较小，学习的近似误差会减小，估计误差会增大，预测结果会对近邻的实例点非常敏感，如果近邻的实例点恰巧是噪声，预测就会出错，k值的减小意味着整体模型变得复杂，容易发生过拟合.  
如果选择的K较大，可以减少学习的估计误差，但是近似误差会增大，输入实例较远的训练实例也会对预测起作用，k值的增大意味着整体模型变得简单。  
训练中k值一般选取一个比较小的数值，通常采用交叉验证法来选取最优的k值

## 分类决策规则
k近邻法中的分类决策规则往往是**多数表决**，即由输入实例的k个近邻的训练实例中的多数类决定输入实例的类，多数表决规则有如下解释:如果分类的损失函数为0-1损失函数，分类函数为
$$f:R^{n}->\{c_{1},c_{2},···,c_{k}\}$$
那么误分类的概率是
$$P(Y\neq f(X))=1-P(Y=f(X))$$
对于给定的实例x∈X，其最近邻的k个训练实例点构成集合$N_{k}(x)$,如果涵盖$N_{k}(x)$的区域的类别是$c_{j}$，那么误分类率是
$$\frac{1}{k}\sum _{x_{i}∈N_{k}(x)}I(y_{i}\neq c_{j})=1-\frac{1}{k}\sum _{x_{i}∈N_{k}(x)}I(y_{i}=c_{j})$$
要使误分类率最小即经验风险最小，就要使$\sum _{x_{i}∈N_{k}(x)}I(y_{i}=c_{j})$最大，所以多数表决规则等价于经验风险最小化
## k近邻法的实现:kd树
实现k近邻法时，主要考虑如何多训练数据进行快速k近邻搜索，最简单的实现是**线性扫描**,非常耗时，为了减少计算距离的次数，常用的是**kd树**
#### 构造kd树
kd树是一种对k维空间中的实例点进行存储以便快速检索的树形数据结构，kd树是**二叉树**，是对k维空间的一个划分，构造kd树相当于不断地用垂直于坐标轴的超平面将k维空间切分，构成一系列的超矩形区域。**kd树的每个结点对应于一个k维超矩形区域**
构造kd‘树的算法  
(1) 开始：构造根节点，根节点对应于包含T的k维空间的超矩形区域，选择x1为坐标轴，以T中所有势力的x1坐标的中位数为且分点，将根节点对应的超矩形区域切分为两个子区域，切分由通过且分点并与坐标轴x1垂直的超平面实现  
由根节点生成深度为1的左、右子节点，左子节点对应坐标x1小于且分点的子区域，右子节点对应于x1大于切分点的子区域。将落在区分超平面上的实例点保存在根节点
(2)重复: 对深度为j的节点，选择$x^{l}$为切分的坐标轴，l=j(modk)+1,以该节点的区域中所有实例的$x^{l}$坐标的中位数为且分点，将该节点对应的超矩形区域切分为两个子区域，切分由通过且分点并与坐标轴$x^{l}$垂直的超平面实现
(3) 直到两个子区域没有实例存在时停止，从而形成kd树的区域划分

#### 搜索kd树
以最近邻为例，给定一个目标点，搜索其最近邻，首先找到包含目标节点的叶节点，然后从该叶节点出发，依次退回到父节点，不断查找到与目标点最邻近的节点，当确定不可能存在更近的节点时终止。


