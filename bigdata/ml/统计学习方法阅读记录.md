# 统计学习方法阅读记录

## 第一章 统计学习方法概论

### 统计学习的对象

**统计学习**的对象是数据，从数据出发，提取数据特征，抽象数据模型，发现数据中的知识，又回到对数据的分析与预测中
统计学习的数据是*同一类的具有一定规律的数据*
### 统计学习的目的
考虑学习什么样的模型和如何学习模型，使模型能对数据进行准确的预测与分析
### 统计学习的方法
包括**模型的假设空间、模型选择的准则以及模型学习的方法**，称其为统计学习三要素，即**模型、策略和算法**
步骤:

* 得到有限的训练数据集合  
* 确定包含所有可能的模型的假设空间，即学习模型的集合  
* 确定模型选择的准则，即学习的策略  
* 实现求解最优模型的算法，即学习策略  
* 通过学习方法选择最优模型  
* 利用学习的最优模型对新数据进行分析

### 监督学习
三个概念**输入空间、特征空间与输出空间**  

监督学习中，将输入与输出看做是定义在输入空间与输出空间上的随机变量的取值。
$X^{i}$通常表示X的第i个特征 $X_{i}$通常表示X的第i个输入变量  

训练数据由输入(或者特征向量)输出对组成  

根据输入输出变量的不同类型，一般求解的问题不一样，输入变量与输出变量为连续变量的预测问题称为**回归问题**，输出变量为有限个离散变量的预测问题称为**分类问题**；输出变量与输出变量均为变量序列的预测问题称为**标注问题**  
监督学习的目的在于学习一个由输入到输出的映射，这一映射由模型来表示，学习的目的就在于找到最好的这样的模型，模型属于由输入空间到输出空间的映射的集合，这个集合就是**假设空间**，假设空间的确定，以为着**学习范围**的确定
### 统计学习三要素
#### 模型
统计学习方法都是由**模型、策略**和**算法**构成  
统计学习首先要考虑的问题是*学习什么样的模型*，在监督学习中，模型就是所要学习的条件概率分布
**P(Y|X)**或决策函数**Y=f(X)**,假设空间用$\Gamma $,表示  
```*条件概率是指事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为：**P（A|B）**```
$$\Gamma={f|Y=f_{\Theta}(X),\Theta∈R^{n}}$$
参数向量$\Theta$取值与n维欧式空间$R^{n}$,称为参数空间
####策略
统计学习的目标在于从假设空间$\Gamma$ 中选取最优的模型，引入**损失函数**与**风险函数**的概念，损失函数度量模型一次预测的好坏，风险函数度量平均意义下模型预测的好坏  
损失函数是f(X)和Y的**非负实值函数**，记作L(Y,f(X))

* 常用的损失函数有以下几种:  
（1）0-1损失函数
$$L(Y,f(X))=\left\{\begin{matrix}
1,Y\neq f(x)
\\2,Y= f(x)
\end{matrix}\right.$$  
(2) 平均损失函数
$$L(Y,f(X))=(Y-f(X))^2$$  
(3) 绝对损失函数
$$L(Y,f(X))=|Y-f(X)|$$
(4)对数损失函数或对数似然损失函数
$$L(Y,f(X))=-logP(Y|X)$$



> *在概率论和统计学中，数学期望(mean)（或均值，亦简称期望）是试验中每次
可能结果的概率乘以其结果的总和，是最基本的数学特征之一。它反映随机变量平均取值的大小  
联合概率分布简称联合分布，是两个及以上随机变量组成的
随机变量的概率分布。根据随机变量的不同，联合概率分布
的表示形式也不同。对于离散型随机变量，联合概率分布可
以以列表的形式表示，也可以以函数的形式表示；对于连续
型随机变量，联合概率分布通过非负函数的积分表示  
联合概率分布简称联合分布，随机向量$X=(X_{1},X_{2},X_{3},···X_{n})$的概率分布，称为随机变量$X_{1},X_{2},X_{3},···X_{n}$的联合概率分布*

期望风险$R_{exp}(f)$是模型关于联合分布的期望损失，经验风险$R_{emp}$是模型关于训练样本集的平均损失，当样本容量N趋于无穷时，经验风险趋于期望风险，但是由于训练样本数目有限，要对经验风险进行一定的矫正，关系到监督学习的两个基本策略:**经验风险最小化与结构风险最小化**
经验风险最小化(ERM)的策略认为经验风险最小的模型就是最优的模型，按照经验风险最小化求最优模型就是求解最优化问题，其定义为:
$$\underset{f∈\Gamma }{min}\frac{1}{N}\sum_{i=1}^{n}L(y_{i},f(X))$$
其中$\Gamma$是假设空间，当样本容量足够大时，经验风险最小化能保证有很好的学习效果，**极大似然估计**就是经验风险最小化的一个例子，当模型是条件概率分布，损失函数是对数损失函数时，经验风险最小化就是极大似然估计。
**结构风险最小化(SRM)**是为了防止过拟合而提出的，结构风险最小化等价于**正则化**，定义:
$$R_{srm}(f)=\frac{1}{N}\sum_{i=1}^{n}L(y_{i},f(X))+\lambda J(f)$$
其中J(f)为模型的复杂度，是定义在假设空间$\Gamma$上的泛函，模型f越复杂，复杂度J(f)就越大，复杂度表示了度复杂模型的惩罚，$\lambda\geqslant 0$是系数，用以权衡经验风险和模型复杂度，结构风险小需要经验风险与模型复杂度同时小，当模型是条件概率分布,损失函数是对数损失函数，模型复杂度由模型的先验概率表示时，结构风险最小化就等价于**最大后验概率估计**  
求最优模型就是求解最优化问题:
$$\underset{f∈\Gamma }{min}\frac{1}{N}\sum_{i=1}^{n}L(y_{i},f(X))+\lambda J(f)$$
**这样监督学习问题就变成了经验风险或结构风险函数的最优化问题，这是经验或结构风险函数是最优化的目标函数**
> 泛函通常是指一种定义域为函数，而值域为实数的“函数”。换句话说，就是从函数组成的一个向量空间到实数的一个映射。也就是说它的输入为函数，而输出为实数。

#### 算法
算法是指学习模型的具体计算方法。
### 模型评估与模型选择
**训练误差** 假设学习到的模型是$Y=\hat{f}(X)$，训练误差是模型$Y=\hat{f}(X)$关于训练数据集的平均损失:
$$R_{emp}(\hat(f))=\frac{1}{N}\sum_{i=1}^{N}L(y_{i},f(X))$$
**测试误差是模型$Y=\hat{f}(X)$关于测试数据集的平均误差**  
例如损失函数是0-1损失函数时，测试误差就编程了长江的测试数据集上的误差率
$$e_{test}=\frac{1}{N}\sum_{i=1}^{N}I(y_{i}\neq \hat{f}(x_{i}))$$
这里I是指示函数，即$y\neq \hat{f}(x)$时为1，否则是0
**通常将学习方法对于未知数据的预测能录称为泛化能力**  
**过拟合** 如果一味追求提高对训练数据的预测能力，所选模型的复杂度往往会比真模型更高，这种现象称为过拟合。指学习时选择的模型所包含的参数过滤，以至于出现这一模型对已知数据预测得很好，但是对位置数据预测很差的现象。  
模型选择方法 **正则化与交叉验证**  
**正则化**是结构风险最小化策略的实现，是在经验风险上加一个正则化项或罚项，**正则化项**一般是模型复杂度的单调递增函数。 
正则化一般具有如下形式
$$\underset{f∈\Gamma }{min}\frac{1}{N}\sum_{i=1}^{n}L(y_{i},f(X))+\lambda J(f)$$
**交叉验证** 重复使用数据，吧给定的数据进行切分，将切分的数据集组合为训练集与测试机，再次基础上反复的进行训练、测试以及模型选择  
一般由如下方式:简单交叉验证、S折交叉验证、留一交叉验证
### 泛化能力
####泛化误差
**泛化能力** 指由该方法学习到的模型对未知数据的预测能力  
**泛化误差** 如果学到的模型是f 那么欧诺个这个模型对位置数据预测的误差即为泛化误差
$$R_{exp}(f)=E_{p}[L(Y,F(X))]=\int_{x*y}L(y,f(x))P(x,y)dxdy$$
**事实上泛化误差就是所学习到的模型的期望风险**
通常通过比较两种学习方法的泛化误差上界的大小来比较他们的优劣  
例子:二类分类问题的泛化误差上界  
**对二分类问题，当假设空间是有限个函数的集合$F={f_{1},f_{2},...f_{d}}$**,对任意一个函数$f∈F$,至少以概率1-$\delta $,以下等式成立
$$R(f)\leq R(f)+\varepsilon (d,N,\delta)$$

### 生成模型与判别模型
监督学习的任务一般就是学习一个模型，模型的一般形式为决策函数或者条件概率分布，监督学习方法可以分为生成方法和判别方法。所学到的模型分别称为生成模型和判别模型  
**生成方法**由数据学习联合概率分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型:
$$P(Y|X)=\frac{P(X,Y)}{P(X)}$$
方法称为生成方法，是因为模型表示了给定输入X产生输出Y的生成关系，典型的生成模型:**朴素贝叶斯发和隐马尔科夫模型**  
**判别方法**由数据直接学习决策函数f(X)或者条件概率分布P(Y|X)作为预测的模型，判别方法关系的是给定的输入X，应该预测什么样的输出Y，典型的判别模型:**K近邻、感知机、决策树**
生成方法的特点:可以还原出联合概率分布P(X,Y)，学习的收敛速度更快，即当样本容量增加时学到的模型可以更快的收敛于真实模型，存在隐变量时可以使用生成方法，不能使用判别方法  
判别方法特点: 直接学习的是条件概率P(Y|X)或者决策函数f(X),直接面对预测，往往准确率更高，可以对数据进行各种程度上的抽象，定义特征并使用特征
### 分类问题
当输出变量Y取有限个离散值，预测问题便成为分类问题，学习到的分类模型或者决策函数称为分类器，分类的类别为多个时，称为多类分类问题  
评价分类性能指标一本是分类准确率，常用评价指标是精确率和召回率。分类情况

* TP--将正类预测为正类数
* FN--将正类预测为负类数
* FP--将负类预测为正类数
* TN--将负类预测为负类数

精确率 $P=\frac{TP}{TP+FP}$
召回率 $R=\frac{TP}{TP+FN}$

### 标注问题
标注问题的输入是一个观测序列，输出是一个标记序列或者状态序列，标准问题的目标在于学习一个模型，使它能对观测序列给出标记序列作为预测，可能的标记个数是有限的。  
标注常用的统计学习方法有: 隐马尔科夫模型、条件随机场
自然语言中的词性标注是一个典型的标注问题(感觉是预测模型加上分类模型)

### 回归问题
回归用于预测输入变量和输出变量之间的关系，特别是当输入变量的值放生变化时，输出变量的值随之发生的变化，回归模型表示从输入变量到输出变量之间映射的函数。回归问题等价于学习**函数拟合**，按照输入变量个数分为一元回归和多元回归，按照输入变量和输出变量之间的关系类型分为线性回归和非线性回归，最常用的损失函数是**平方损失函数**，在此请情况下可以由最小二乘法求解












